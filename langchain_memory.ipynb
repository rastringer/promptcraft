{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rastringer/promptcraft/blob/main/langchain_memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ii3ihksojvA"
      },
      "source": [
        "### Memory\n",
        "\n",
        "\n",
        "In many applications, it is essential LLMs remember prior interactions and context.\n",
        "\n",
        "Langchain provides several helper functions to manage and manipulate previous chat messages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2k24lCDonBx"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade google-cloud-aiplatform\n",
        "! pip install shapely<2.0.0\n",
        "! pip install langchain\n",
        "! pip install pypdf\n",
        "! pip install pydantic==1.10.8\n",
        "! pip install langchain[docarray]\n",
        "! pip install typing-inspect==0.8.0 typing_extensions==4.5.0\n",
        "# Hugging Face transformers necessary for ConversationTokenBufferMemory\n",
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khvp_6yndlyD"
      },
      "source": [
        "This optional cell wraps outputs, which can make them easier to digest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqKE27cqZBk2"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPxAFOghonE3"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN7KknySjqov"
      },
      "source": [
        "If you're on Colab, authenticate via the following cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BrhaYTBor1M"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqRMCpihjvxY"
      },
      "source": [
        "### Initialize the SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAn62-Fcottw"
      },
      "outputs": [],
      "source": [
        "# Add your project id and the project's region\n",
        "PROJECT_ID = \"<...>\"\n",
        "REGION = \"<...>\"\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7Y7McwEz8Zk"
      },
      "outputs": [],
      "source": [
        "# Utils\n",
        "import time\n",
        "from typing import List\n",
        "\n",
        "# Langchain\n",
        "import langchain\n",
        "from pydantic import BaseModel\n",
        "\n",
        "print(f\"LangChain version: {langchain.__version__}\")\n",
        "\n",
        "# Vertex AI\n",
        "from google.cloud import aiplatform\n",
        "from langchain.chat_models import ChatVertexAI\n",
        "from langchain.llms import VertexAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPtkjVdbdJfR"
      },
      "outputs": [],
      "source": [
        "# LLM model\n",
        "llm = VertexAI(\n",
        "    model_name=\"text-bison@001\",\n",
        "    max_output_tokens=256,\n",
        "    temperature=0.1,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG1tXRyjdrmW"
      },
      "source": [
        "### ConversationBufferWindowMemory\n",
        "Keeps a list of the interactions of the conversation over time. It only uses the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAaOjxI849xs"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=3)\n",
        "\n",
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"How are you?\"})\n",
        "memory.save_context({\"input\": \"Fine thanks\"},\n",
        "                    {\"output\": \"Great\"})\n",
        "\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXpMUfwbdeF2"
      },
      "source": [
        "### ConversationTokenBufferMemory\n",
        "Keeps a buffer of recent interactions in memory, and uses token length rather than number of interactions to determine when to flush interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ3e-s405W1g"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "\n",
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=100)\n",
        "memory.save_context({\"input\": \"All alone, she dreams of the stars!\"},\n",
        "                    {\"output\": \"As she should!\"})\n",
        "memory.save_context({\"input\": \"Baking cookies today?\"},\n",
        "                    {\"output\": \"Behold the cookies!\"})\n",
        "memory.save_context({\"input\": \"Chatbots everywhere?\"},\n",
        "                    {\"output\": \"Certainly!\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBWcRil85gtx"
      },
      "outputs": [],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D312u4rWeBd8"
      },
      "source": [
        "In this example, we experiment with summarising the conversation at `max_token_limit`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkZGBKFz84-A"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "conversation_with_summary = ConversationChain(\n",
        "    llm=llm,\n",
        "    # We set a very low max_token_limit for the purposes of testing.\n",
        "    memory=ConversationTokenBufferMemory(llm=llm, max_token_limit=60),\n",
        "    verbose=True,\n",
        ")\n",
        "conversation_with_summary.predict(input=\"Hi, how are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNflvyyTeTjT"
      },
      "source": [
        "### ConversationSummaryBufferMemory\n",
        "\n",
        "Ensures conversational memory endures by summarizing old interactions to help inform chat within a new window. It uses token length to determine when to 'flush' the interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YDPUKhP9Xuy"
      },
      "outputs": [],
      "source": [
        "conversation_with_summary.predict(input=\"I'm working on learning C++\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPXS_h019bnT"
      },
      "outputs": [],
      "source": [
        "conversation_with_summary.predict(input=\"What's the best book to help me?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zJ1OQRH97Oc"
      },
      "outputs": [],
      "source": [
        "# Notice the buffer here is updated and clears the earlier exchanges\n",
        "conversation_with_summary.predict(input=\"Wish me luck!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o08118VIgdzH"
      },
      "outputs": [],
      "source": [
        "conversation_with_summary.predict(input=\"Would knowing C help me?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94tz2BIrfFnv"
      },
      "source": [
        "### ConversationSummaryBufferMemory\n",
        "\n",
        "Ensures conversational memory endures by summarizing old interactions to help inform chat within a new window. It uses token length to determine when to 'flush' the interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hc9XH1GH5wdL"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "\n",
        "# create a long string\n",
        "activities = \"I'm due at the pool for a training session \\\n",
        "with the swim coach. \\\n",
        "Then it's straight out on the bike into the mountains for a 60-miler. \\\n",
        "There will be speed reps in between the mountain climbs. \\\n",
        "The p.m. workout will be ten miles @ 60-70% effort. \\\n",
        "I should need to check the bike tyres and sleep well tonight to prepare for \\\n",
        "the training session.\"\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=30)\n",
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "memory.save_context({\"input\": \"What training is on today?\"},\n",
        "                    {\"output\": f\"{activities}\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WK0yh6Hb5t0i"
      },
      "outputs": [],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u8gKd1dhAHq"
      },
      "outputs": [],
      "source": [
        "messages = memory.chat_memory.messages\n",
        "previous_summary = \"\"\n",
        "memory.predict_new_summary(messages, previous_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIrgS5r453WC"
      },
      "outputs": [],
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxqYayPFhio_"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"Hi, what's up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBifcEOuhmYV"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"Not much, resting while I can\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Vqe79Yn551T"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"What should I do to prepare for the training session?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNNEqJTphxwA"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"What does the run session look like?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylWRS7mk57UD"
      },
      "outputs": [],
      "source": [
        "# The memory keeps the storage of the conversation\n",
        "# up to the specified 30 token limit\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rkzR1SYAH09"
      },
      "source": [
        "### Summary\n",
        "\n",
        "In this notebook, we explored various approaches to memory in conversations.\n",
        "\n",
        "* ConversationBufferWindowMemory\n",
        "\n",
        "* ConversationSummaryBufferMemory\n",
        "\n",
        "* ConversationTokenBufferMemory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iezD_X2d_lee"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMU5+DWjnQQhIrQT4X559MU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}