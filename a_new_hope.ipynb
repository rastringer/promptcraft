{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rastringer/promptcraft_notebooks/blob/main/a_new_hope.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ii3ihksojvA"
      },
      "source": [
        "## Talk to your Data: Star Wars\n",
        "\n",
        "In this notebook, we will embed the script for the 1978 Star Wars film: \"A New Hope\", then use Vertex AI language models to 'chat' with the data.\n",
        "\n",
        "We will use the following technologies:\n",
        "\n",
        "* Vertex AI Generative Studio\n",
        "\n",
        "* Langchain, a framework for building applications with large language models\n",
        "\n",
        "* The open-source Chroma vector store database\n",
        "\n",
        "We will apply the following approaches:\n",
        "\n",
        "* Retrieval Augmented Generation (RAG). Using RAG, we feed the model and ask it to inform its answers based on the details in the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvX9sxdsgTJW"
      },
      "source": [
        "<img src=\"https://github.com/rastringer/promptcraft_notebooks/blob/main/images/panel_demo.png?raw=1\" alt=\"Panel demo\" width=\"500\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFgmcxXOIhPD"
      },
      "source": [
        "### What is an embedding?\n",
        "\n",
        "To feed text, image or audio to machine learning models, we first have to convert it to numerical values a model can understand.\n",
        "\n",
        "Embeddings in this example convert the text in the film script into floating point numbers that denote similarity. We accomplish this by using a trained model (from Vertex) that knows \"Lightsaber\" and \"Jedi\" should be close together in the 'embedding space'. This means we can embed the script and preserve the similarity scores of the words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ravTJRUsKS_Y"
      },
      "source": [
        "<img src=\"https://github.com/rastringer/promptcraft_notebooks/blob/main/images/embeddings.png?raw=1\" alt=\"Embeddings\" width=\"600\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Application flow\n",
        "\n",
        "<img src=\"https://github.com/rastringer/promptcraft_notebooks/blob/main/images/flow.png?raw=1\" alt=\"Flow\" width=\"600\"/>"
      ],
      "metadata": {
        "id": "4rIYe0-63jOQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "am33TYRmgTJX"
      },
      "outputs": [],
      "source": [
        "# Install the packages\n",
        "! pip3 install --upgrade google-cloud-aiplatform\n",
        "! pip3 install shapely<2.0.0\n",
        "! pip install langchain\n",
        "! pip install pypdf\n",
        "! pip install pydantic==1.10.8\n",
        "! pip install chromadb==0.3.26\n",
        "! pip install langchain[docarray]\n",
        "! pip install typing-inspect==0.8.0 typing_extensions==4.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPxAFOghonE3"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BrhaYTBor1M"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SDK and Project Initialization"
      ],
      "metadata": {
        "id": "qn8IZiXWgyXz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAn62-Fcottw"
      },
      "outputs": [],
      "source": [
        "#Fill in your GCP project_id and region\n",
        "PROJECT_ID = \"<>\"\n",
        "REGION = \"<>\"\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Langchain tools"
      ],
      "metadata": {
        "id": "1qUJ7u3Ag8nh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHBD6TZGfs8X"
      },
      "outputs": [],
      "source": [
        "# Utils\n",
        "import time\n",
        "from typing import List\n",
        "\n",
        "# Langchain\n",
        "import langchain\n",
        "from pydantic import BaseModel\n",
        "\n",
        "print(f\"LangChain version: {langchain.__version__}\")\n",
        "\n",
        "# Vertex AI\n",
        "from google.cloud import aiplatform\n",
        "from langchain.chat_models import ChatVertexAI\n",
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "from langchain.llms import VertexAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import data"
      ],
      "metadata": {
        "id": "uMGTzcdKzObI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://assets.scriptslug.com/live/pdf/scripts/star-wars-episode-iv-a-new-hope-1977.pdf"
      ],
      "metadata": {
        "id": "iRtvCj3ujmaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSi2BILyHhUz"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import VertexAI\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# Copy the file path of the downloaded script.\n",
        "# In Colab, it should appear as below.\n",
        "loader = PyPDFLoader(\"/content/star-wars-episode-iv-a-new-hope-1977.pdf\")\n",
        "\n",
        "doc = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text splitters\n",
        "\n",
        "Language models often constrain the amount of text that can be fed as an input, so it is good practice to use text splitters to keep inputs to manageable 'chunks'.\n",
        "\n",
        "We can also often improve results from vector store matches since smaller chunks may be more likely to match queries."
      ],
      "metadata": {
        "id": "l-aNNwlLtS60"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xt3tUI5rH8-k"
      },
      "outputs": [],
      "source": [
        "# Split\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1500,\n",
        "    chunk_overlap = 150\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAShiGIlJyAJ"
      },
      "outputs": [],
      "source": [
        "splits = text_splitter.split_documents(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ADWTyOUJz6o"
      },
      "outputs": [],
      "source": [
        "len(splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8D-EooYnKIt6"
      },
      "outputs": [],
      "source": [
        "from vertexai.preview.language_models import TextEmbeddingModel\n",
        "\n",
        "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings example\n",
        "\n",
        "As a simple example of embedding sentences, we will use the Vertex AI SDK and embedding model to work out numerical values for some simple sentences.\n",
        "\n",
        "We then calculate the dot product of the resulting arrays of floats. Sentences that are similar should have higher dot product results."
      ],
      "metadata": {
        "id": "DJ0DtSAct5vJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FelN1n_QNJ88"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def text_embedding() -> None:\n",
        "    \"\"\"Text embedding with a Large Language Model.\"\"\"\n",
        "    model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
        "    embeddings1 = model.get_embeddings([\"I like dogs\"])\n",
        "    embeddings2 = model.get_embeddings([\"Canines are my favourite\"])\n",
        "    embeddings3 = model.get_embeddings([\"What is life?\"])\n",
        "    for embedding in embeddings1:\n",
        "        vector1 = embedding.values\n",
        "    for embedding in embeddings2:\n",
        "        vector2 = embedding.values\n",
        "    for embedding in embeddings3:\n",
        "        vector3 = embedding.values\n",
        "    print(f\"Dot product of sentence1 and sentence2: {np.dot(vector1, vector2)}\")\n",
        "    print(f\"Dot product of sentence1 and sentence3: {np.dot(vector1, vector3)}\")\n",
        "    # print(f\"Length of Embedding Vector: {len(vector)}\")\n",
        "    # print(vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rskl7Jg1NLs3"
      },
      "outputs": [],
      "source": [
        "text_embedding()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXThUJTjQK1l"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# Clear any previous vector store\n",
        "!rm -rf ./docs/chroma"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set up a vector database using the open source [Chroma](https://www.trychroma.com/)."
      ],
      "metadata": {
        "id": "QNBRbBWfuXbQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Upnhgwyxmn6W"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "\n",
        "persist_directory = 'docs/chroma/'\n",
        "embeddings = VertexAIEmbeddings()\n",
        "\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=splits[0:4],\n",
        "    embedding=embeddings,\n",
        "    persist_directory=persist_directory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNx4zWvomp68"
      },
      "outputs": [],
      "source": [
        "print(vectordb._collection.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeAGxPIpoPYK"
      },
      "outputs": [],
      "source": [
        "question = \"Who is Luke Skywalker?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idSPiLNzoZiV"
      },
      "outputs": [],
      "source": [
        "# Here, k=3 specifies the number of relevant documents we want to return\n",
        "docs = vectordb.similarity_search(question,k=3)\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# As requested, we get three docs from the similarity search\n",
        "len(docs)"
      ],
      "metadata": {
        "id": "zbpUJs3yvDuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cn-sa7zCo_um"
      },
      "outputs": [],
      "source": [
        "question = \"who is han solo?\"\n",
        "docs_ss = vectordb.similarity_search(question,k=3)\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfJTJ5Hcooal"
      },
      "outputs": [],
      "source": [
        "len(docs_ss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "araK8nelpt0F"
      },
      "outputs": [],
      "source": [
        "question = \"What are the rebel alliance's chance against the empire?\"\n",
        "docs = vectordb.similarity_search(question,k=3)\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[1].page_content)"
      ],
      "metadata": {
        "id": "pkSkXL5mvlGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieval"
      ],
      "metadata": {
        "id": "79kVcgDjwHBx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcbs0BxbqV2o"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "llm = VertexAI(\n",
        "    model_name=\"text-bison@001\",\n",
        "    max_output_tokens=1024,\n",
        "    temperature=0.1,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d7haWcwrDkG"
      },
      "source": [
        "### Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WvFmNANrEFm"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Build prompt\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. \\\n",
        "If you don't know the answer, just say that you don't know, \\\n",
        "don't try to make up an answer. Use six sentences maximum. \\\n",
        "Keep the answer as concise as possible.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5pVrtJBrEct"
      },
      "outputs": [],
      "source": [
        "# Run chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who is Luke Skywalker?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ],
      "metadata": {
        "id": "lhE2wYNgwu2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking for hallucinations\n"
      ],
      "metadata": {
        "id": "U7i8DKwWwtRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is Darth Vader's favourite Spotify playlist?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ],
      "metadata": {
        "id": "hgJInDf-wjde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AzCZcJhrHp2"
      },
      "outputs": [],
      "source": [
        "question = \"How does Obi Wan know Darth Vader?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-oozCA4tnaM"
      },
      "source": [
        "### Chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oD-l4U91toMR"
      },
      "outputs": [],
      "source": [
        "# Build prompt\n",
        "from langchain.prompts import PromptTemplate\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. \\\n",
        "If you don't know the answer, just say that you don't know, \\\n",
        "don't try to make up an answer.  \\\n",
        "Use four sentences maximum.  \\\n",
        "Write with the enthusiasm of a true fan for the material. \\\n",
        "Add detail to your answers from the story.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)\n",
        "\n",
        "# Run chain\n",
        "from langchain.chains import RetrievalQA\n",
        "question = \"What are the major topics in the film?\"\n",
        "qa_chain = RetrievalQA.from_chain_type(llm,\n",
        "                                       retriever=vectordb.as_retriever(),\n",
        "                                       return_source_documents=True,\n",
        "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
        "\n",
        "\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzdaD7e8t0kj"
      },
      "source": [
        "### Memory\n",
        "\n",
        "For an effective chat, we need the model to remember its previous responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O88jPynFtuGD"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHlD2lA_t1rZ"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "retriever=vectordb.as_retriever()\n",
        "qa = ConversationalRetrievalChain.from_llm(\n",
        "    llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyO67EcRt3PD"
      },
      "outputs": [],
      "source": [
        "question = \"Does Obi Wan know Darth Vader?\"\n",
        "result = qa({\"question\": question})\n",
        "result['answer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4saT_HKjt9Mj"
      },
      "outputs": [],
      "source": [
        "question = \"How?\"\n",
        "result = qa({\"question\": question})\n",
        "result[\"answer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvUN-W9EuBPj"
      },
      "outputs": [],
      "source": [
        "question = \"Why did they cease to be friends?\"\n",
        "result = qa({\"question\": question})\n",
        "result[\"answer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONSKWUR2uCf8"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.chains import RetrievalQA,  ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chat_models import ChatVertexAI\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiONoCl9uFmE"
      },
      "outputs": [],
      "source": [
        "def load_db(file, chain_type, k):\n",
        "    # load documents\n",
        "    loader = PyPDFLoader(file)\n",
        "    documents = loader.load()\n",
        "    # split documents\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "    # define embedding\n",
        "    embeddings = VertexAIEmbeddings()\n",
        "    # create vector database from data\n",
        "    db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
        "    # define retriever\n",
        "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
        "    # create a chatbot chain. Memory is managed externally.\n",
        "    qa = ConversationalRetrievalChain.from_llm(\n",
        "        llm=VertexAI(temperature=0.1, max_output_tokens=1024),\n",
        "        chain_type=chain_type,\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True,\n",
        "        return_generated_question=True,\n",
        "    )\n",
        "    return qa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBbFS3LtuIEU"
      },
      "outputs": [],
      "source": [
        "import panel as pn\n",
        "import param\n",
        "\n",
        "class cbfs(param.Parameterized):\n",
        "    chat_history = param.List([])\n",
        "    answer = param.String(\"\")\n",
        "    db_query  = param.String(\"\")\n",
        "    db_response = param.List([])\n",
        "\n",
        "    def __init__(self,  **params):\n",
        "        super(cbfs, self).__init__( **params)\n",
        "        self.panels = []\n",
        "        self.loaded_file = \"/content/star-wars-episode-iv-a-new-hope-1977.pdf\"\n",
        "        self.qa = load_db(self.loaded_file,\"stuff\", 4)\n",
        "\n",
        "    def call_load_db(self, count):\n",
        "        if count == 0 or file_input.value is None:  # init or no file specified :\n",
        "            return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
        "        else:\n",
        "            file_input.save(\"temp.pdf\")  # local copy\n",
        "            self.loaded_file = file_input.filename\n",
        "            button_load.button_style=\"outline\"\n",
        "            self.qa = load_db(\"temp.pdf\", \"stuff\", 4)\n",
        "            button_load.button_style=\"solid\"\n",
        "        self.clr_history()\n",
        "        return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
        "\n",
        "    def convchain(self, query):\n",
        "        if not query:\n",
        "            return pn.WidgetBox(pn.Row('User:', pn.pane.Markdown(\"\", width=600)), scroll=True)\n",
        "        result = self.qa({\"question\": query, \"chat_history\": self.chat_history})\n",
        "        self.chat_history.extend([(query, result[\"answer\"])])\n",
        "        self.db_query = result[\"generated_question\"]\n",
        "        self.db_response = result[\"source_documents\"]\n",
        "        self.answer = result['answer']\n",
        "        self.panels.extend([\n",
        "            pn.Row('User:', pn.pane.Markdown(query, width=600)),\n",
        "            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=600))\n",
        "        ])\n",
        "        inp.value = ''  #clears loading indicator when cleared\n",
        "        return pn.WidgetBox(*self.panels,scroll=True)\n",
        "\n",
        "    @param.depends('db_query ', )\n",
        "    def get_lquest(self):\n",
        "        if not self.db_query :\n",
        "            return pn.Column(\n",
        "                pn.Row(pn.pane.Markdown(f\"Last question to DB:\")),\n",
        "                pn.Row(pn.pane.Str(\"no DB accesses so far\"))\n",
        "            )\n",
        "        return pn.Column(\n",
        "            pn.Row(pn.pane.Markdown(f\"DB query:\")),\n",
        "            pn.pane.Str(self.db_query )\n",
        "        )\n",
        "\n",
        "    @param.depends('db_response', )\n",
        "    def get_sources(self):\n",
        "        if not self.db_response:\n",
        "            return\n",
        "        rlist=[pn.Row(pn.pane.Markdown(f\"Result of DB lookup:\"))]\n",
        "        for doc in self.db_response:\n",
        "            rlist.append(pn.Row(pn.pane.Str(doc)))\n",
        "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
        "\n",
        "    @param.depends('convchain', 'clr_history')\n",
        "    def get_chats(self):\n",
        "        if not self.chat_history:\n",
        "            return pn.WidgetBox(pn.Row(pn.pane.Str(\"No History Yet\")), width=600, scroll=True)\n",
        "        rlist=[pn.Row(pn.pane.Markdown(f\"Current Chat History variable\"))]\n",
        "        for exchange in self.chat_history:\n",
        "            rlist.append(pn.Row(pn.pane.Str(exchange)))\n",
        "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
        "\n",
        "    def clr_history(self,count=0):\n",
        "        self.chat_history = []\n",
        "        return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUemuRXWuLWU"
      },
      "outputs": [],
      "source": [
        "pn.extension()\n",
        "\n",
        "cb = cbfs()\n",
        "\n",
        "file_input = pn.widgets.FileInput(accept='.pdf')\n",
        "button_load = pn.widgets.Button(name=\"Load DB\", button_type='primary')\n",
        "button_clearhistory = pn.widgets.Button(name=\"Clear History\", button_type='warning')\n",
        "button_clearhistory.on_click(cb.clr_history)\n",
        "inp = pn.widgets.TextInput( placeholder='Enter text hereâ€¦')\n",
        "\n",
        "bound_button_load = pn.bind(cb.call_load_db, button_load.param.clicks)\n",
        "conversation = pn.bind(cb.convchain, inp)\n",
        "\n",
        "tab1 = pn.Column(\n",
        "    pn.Row(inp),\n",
        "    pn.layout.Divider(),\n",
        "    pn.panel(conversation,  loading_indicator=True, height=300),\n",
        "    pn.layout.Divider(),\n",
        ")\n",
        "tab2= pn.Column(\n",
        "    pn.panel(cb.get_lquest),\n",
        "    pn.layout.Divider(),\n",
        "    pn.panel(cb.get_sources ),\n",
        ")\n",
        "tab3= pn.Column(\n",
        "    pn.panel(cb.get_chats),\n",
        "    pn.layout.Divider(),\n",
        ")\n",
        "tab4=pn.Column(\n",
        "    pn.Row( file_input, button_load, bound_button_load),\n",
        "    pn.Row( button_clearhistory, pn.pane.Markdown(\"Clears chat history. Can use to start a new topic\" )),\n",
        "    pn.layout.Divider(),\n",
        ")\n",
        "dashboard = pn.Column(\n",
        "    pn.Row(pn.pane.Markdown('# Chat with your data')),\n",
        "    pn.Tabs(('Conversation', tab1), ('Database', tab2), ('Chat History', tab3),('Configure', tab4))\n",
        ")\n",
        "dashboard"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With thanks to Deeplearning.ai's excellent [LangChain Chat With Your Data](https://learn.deeplearning.ai/langchain-chat-with-your-data/lesson/1/introduction) course."
      ],
      "metadata": {
        "id": "PKiyOytdyNyo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZIRMdeLuOil"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}