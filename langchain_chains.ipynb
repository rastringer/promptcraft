{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rastringer/promptcraft/blob/main/langchain_chains.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ii3ihksojvA"
      },
      "source": [
        "### Chains\n",
        "\n",
        "Complex applications will require chaining LLMs together, or with other components.\n",
        "\n",
        "We will cover the following types of chains:\n",
        "\n",
        "* Sequential chains\n",
        "\n",
        "* Router chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2k24lCDonBx"
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade google-cloud-aiplatform\n",
        "! pip3 install shapely<2.0.0\n",
        "! pip install langchain\n",
        "! pip install pypdf\n",
        "! pip install pydantic==1.10.8\n",
        "! pip install chromadb==0.3.26\n",
        "! pip install langchain[docarray]\n",
        "! pip install typing-inspect==0.8.0 typing_extensions==4.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPxAFOghonE3"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN7KknySjqov"
      },
      "source": [
        "If you're on Colab, authenticate via the following cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BrhaYTBor1M"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqRMCpihjvxY"
      },
      "source": [
        "# Initialize the SDK and LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAn62-Fcottw"
      },
      "outputs": [],
      "source": [
        "# Add your project id and the region\n",
        "PROJECT_ID = \"<..>\"\n",
        "REGION = \"<..>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7Y7McwEz8Zk"
      },
      "outputs": [],
      "source": [
        "# Utils\n",
        "import time\n",
        "from typing import List\n",
        "\n",
        "# Vertex AI\n",
        "import vertexai\n",
        "\n",
        "# Langchain\n",
        "import langchain\n",
        "from pydantic import BaseModel\n",
        "\n",
        "print(f\"LangChain version: {langchain.__version__}\")\n",
        "from langchain.chat_models import ChatVertexAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.llms import VertexAI\n",
        "from langchain.chains import LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LogO0tJ9z-73"
      },
      "outputs": [],
      "source": [
        "vertexai.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "# LLM model\n",
        "llm = VertexAI(\n",
        "    model_name=\"text-bison@001\",\n",
        "    max_output_tokens=256,\n",
        "    # Increasing the temp\n",
        "    # for more creative output\n",
        "    temperature=0.9,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ygg4CI2GJTS"
      },
      "source": [
        "### LLMChain\n",
        "\n",
        "An LLMChain simply provides a prompt to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfBv1FibFnEX"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"What is the best name to describe \\\n",
        "    a company that makes {product}?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dVbmJqTFpmg"
      },
      "outputs": [],
      "source": [
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "product = \"A saw for laminate wood\"\n",
        "chain.run(product)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh0neMCVHEz7"
      },
      "source": [
        "### Sequential chain\n",
        "\n",
        "A sequential chain makes a series of calls to an LLM. It enables a pipeline-style workflow in which the output from one call becomes the input to the next.\n",
        "\n",
        "The two types include:\n",
        "\n",
        "* `SimpleSequentialChain`, where predictably each step has a single input and output, which becomes the input to the next step.\n",
        "\n",
        "* `SequentialChain`, which allows for multiple inputs and outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VouPFJhHIiZz"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUc-2YPcHENV"
      },
      "outputs": [],
      "source": [
        "# This is an LLMChain to write a synopsis given a title of a play.\n",
        "llm = VertexAI(temperature=0.7)\n",
        "template = \"\"\"You are an entrepreneur. Think of a ground breaking new product and write a short pitch.\n",
        "\n",
        "Title: {title}\n",
        "Entrepreneur: This is a pitch for the above product:\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"title\"], template=template)\n",
        "pitch_chain = LLMChain(llm=llm, prompt=prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9ITVEsMIhIm"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"You are a panelist on Dragon's Den. Given a \\\n",
        "description of the product, you are to explain why you think it will \\\n",
        "succeed or fail in the market.\n",
        "\n",
        "Product pitch: {pitch}\n",
        "Review by Dragon's Den panelist:\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"pitch\"], template=template)\n",
        "review_chain = LLMChain(llm=llm, prompt=prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7Of3TaEI_pk"
      },
      "outputs": [],
      "source": [
        "# This is the overall chain where we run these two chains in sequence.\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "overall_chain = SimpleSequentialChain(chains=[pitch_chain, review_chain], verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlMlghzhI_Ev"
      },
      "outputs": [],
      "source": [
        "review = overall_chain.run(\"Portable iced coffee maker\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLI06HNzKwCZ"
      },
      "source": [
        "### Router chain\n",
        "\n",
        "A `RouterChain` dynamically selects the next chain to use for a given input.\n",
        "This feature uses the `MultiPromptChain` to select then answer with the best-suited prompt to the question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jB2aYvTLI_HE"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.router import MultiPromptChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72YmlCrmI_JV"
      },
      "outputs": [],
      "source": [
        "korean_template = \"\"\"\n",
        "You are an expert in korean history and culture.\n",
        "Here is a question:\n",
        "{input}\n",
        "\"\"\"\n",
        "\n",
        "spanish_template = \"\"\"\n",
        "You are an expert in spanish history and culture.\n",
        "Here is a question:\n",
        "{input}\n",
        "\"\"\"\n",
        "\n",
        "chinese_template = \"\"\"\n",
        "You are an expert in Chinese history and culture.\n",
        "Here is a question:\n",
        "{input}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ikwVx6eL6_R"
      },
      "outputs": [],
      "source": [
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"korean\",\n",
        "        \"description\": \"Good for answering questions about Korean history and culture\",\n",
        "        \"prompt_template\": korean_template,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"spanish\",\n",
        "        \"description\": \"Good for answering questions about Spanish history and culture\",\n",
        "        \"prompt_template\": spanish_template,\n",
        "    },\n",
        "     {\n",
        "        \"name\": \"chinese\",\n",
        "        \"description\": \"Good for answering questions about Chinese history and culture\",\n",
        "        \"prompt_template\": chinese_template,\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yj_NGNcwMSBA"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKtB7cVTMSpZ"
      },
      "outputs": [],
      "source": [
        "llm = VertexAI(temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDxchxh-MSrx"
      },
      "outputs": [],
      "source": [
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCg0K4bZMSt_"
      },
      "outputs": [],
      "source": [
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhapHeigMbVS"
      },
      "outputs": [],
      "source": [
        "# Thanks to Deeplearning.ai for this template and for the\n",
        "# Langchain short course at deeplearning.ai/short-courses/.\n",
        "\n",
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising\\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
        "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9vpmJhbMbXp"
      },
      "outputs": [],
      "source": [
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLE68R2LMbaF"
      },
      "outputs": [],
      "source": [
        "chain = MultiPromptChain(router_chain=router_chain,\n",
        "                         destination_chains=destination_chains,\n",
        "                         default_chain=default_chain, verbose=True\n",
        "                        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSKh3QvDN_5v"
      },
      "source": [
        "Notice in the outputs the country of speciality is prefixed eg:\n",
        "`chinese: {'input': ...`, denoting the routing to the correct expert."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKt-VCGkMbcv"
      },
      "outputs": [],
      "source": [
        "|chain.run(\"What was the Han Dynasty?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbzTVq70NN-1"
      },
      "outputs": [],
      "source": [
        "chain.run(\"What are some of the typical dishes in Catalonia?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C10-UADBNOBU"
      },
      "outputs": [],
      "source": [
        "chain.run(\"How would I greet a friend's family in Korean?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQwUaK__NOD8"
      },
      "outputs": [],
      "source": [
        "chain.run(\"Summarize Don Quixote in Spanish?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUTxHhchOLLz"
      },
      "source": [
        "If we provide a question that is outside of our experts' fields, the default model handles it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ToqkAwRN87P"
      },
      "outputs": [],
      "source": [
        "chain.run(\"How can I fix a carburetor?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyN8u9xegQdZTrcF1wNOqGZ7",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
