{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rastringer/promptcraft/blob/main/langchain_memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ii3ihksojvA"
      },
      "source": [
        "### Memory\n",
        "\n",
        "\n",
        "In many applications, it is essential LLMs remember prior interactions and context.\n",
        "\n",
        "Langchain provides several helper functions to manage and manipulate previous chat messages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2k24lCDonBx"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade google-cloud-aiplatform\n",
        "# LangChain\n",
        "! pip install langchain\n",
        "! pip install pypdf\n",
        "! pip install pydantic==1.10.8\n",
        "! pip install typing-inspect==0.8.0 typing_extensions==4.5.0\n",
        "# Hugging Face transformers necessary for ConversationTokenBufferMemory\n",
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "zg2UQrgu3yBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khvp_6yndlyD"
      },
      "source": [
        "This optional cell wraps outputs, which can make them easier to digest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqKE27cqZBk2"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN7KknySjqov"
      },
      "source": [
        "If you're on Colab, authenticate via the following cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BrhaYTBor1M"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqRMCpihjvxY"
      },
      "source": [
        "### Initialize the SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAn62-Fcottw"
      },
      "outputs": [],
      "source": [
        "# Add your project id and the project's region\n",
        "PROJECT_ID = \"<..>\"\n",
        "REGION = \"<..>\"\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7Y7McwEz8Zk"
      },
      "outputs": [],
      "source": [
        "# Utils\n",
        "import time\n",
        "from typing import List\n",
        "\n",
        "# Langchain\n",
        "import langchain\n",
        "from pydantic import BaseModel\n",
        "\n",
        "print(f\"LangChain version: {langchain.__version__}\")\n",
        "\n",
        "# Vertex AI\n",
        "from google.cloud import aiplatform\n",
        "from langchain.chat_models import ChatVertexAI\n",
        "from langchain.llms import VertexAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPtkjVdbdJfR"
      },
      "outputs": [],
      "source": [
        "# LLM model\n",
        "llm = VertexAI(\n",
        "    model_name=\"text-bison@001\",\n",
        "    max_output_tokens=1024,\n",
        "    temperature=0.2,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG1tXRyjdrmW"
      },
      "source": [
        "### ConversationBufferWindowMemory\n",
        "Keeps a list of the interactions of the conversation over time. It only uses the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAaOjxI849xs"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=3)\n",
        "\n",
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"How are you?\"})\n",
        "memory.save_context({\"input\": \"Fine thanks\"},\n",
        "                    {\"output\": \"Great\"})\n",
        "\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXpMUfwbdeF2"
      },
      "source": [
        "### ConversationTokenBufferMemory\n",
        "\n",
        "This feature instead keeps a buffer of recent interactions in memory based on token length, rather than number of interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ3e-s405W1g"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "\n",
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=100)\n",
        "memory.save_context({\"input\": \"All alone, she dreams of the stars!\"},\n",
        "                    {\"output\": \"As she should!\"})\n",
        "memory.save_context({\"input\": \"Baking cookies today?\"},\n",
        "                    {\"output\": \"Behold the cookies!\"})\n",
        "memory.save_context({\"input\": \"Chatbots everywhere?\"},\n",
        "                    {\"output\": \"Certainly!\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBWcRil85gtx"
      },
      "outputs": [],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D312u4rWeBd8"
      },
      "source": [
        "### Conversation summaries\n",
        "\n",
        "LangChain carries forward summaries of chat messages and flushes memory after a specified number of interactions or tokens.\n",
        "\n",
        "Let's first look at using the former, `ConversationBufferWindowMemory`.\n",
        "\n",
        "We set `verbose=True` to show the prompts and information carried forward by the LLM."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "conversation_with_summary = ConversationChain(\n",
        "    llm=VertexAI(temperature=0),\n",
        "    # We set a low k=2, to only keep the last 2 interactions in memory\n",
        "    memory=ConversationBufferWindowMemory(k=2),\n",
        "    verbose=True\n",
        ")\n",
        "conversation_with_summary.predict(input=\"My favourite sport is fencing. Any tips for how I can go pro?\")"
      ],
      "metadata": {
        "id": "zC6mbHV-o0-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_summary.predict(input=\"What equipment do I need?\")\n"
      ],
      "metadata": {
        "id": "rEyXgThio7R4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_summary.predict(input=\"Who are the greats of the sport I can emulate?\")\n"
      ],
      "metadata": {
        "id": "gZfsiZHho-AY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since we have now passed k=2, the LLM will be unable to answer\n",
        "conversation_with_summary.predict(input=\"What is my favourite sport?\")"
      ],
      "metadata": {
        "id": "Mc9aVZ2Go__J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ConversationSummaryBufferMemory\n",
        "\n",
        "Ensures conversational memory up to a specified token length"
      ],
      "metadata": {
        "id": "d7UdIRespJhP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkZGBKFz84-A"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "conversation_with_summary = ConversationChain(\n",
        "    llm=llm,\n",
        "    # Change max_token_limit here after running through the conversation.\n",
        "    memory=ConversationTokenBufferMemory(llm=llm, max_token_limit=400),\n",
        "    verbose=True,\n",
        ")\n",
        "conversation_with_summary.predict(input=\"Hi, how are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNflvyyTeTjT"
      },
      "source": [
        "### ConversationSummaryBufferMemory\n",
        "\n",
        "Ensures conversational memory endures by summarizing old interactions to help inform chat within a new window. It uses token length to determine when to 'flush' the interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YDPUKhP9Xuy"
      },
      "outputs": [],
      "source": [
        "conversation_with_summary.predict(input=\"I'm learning the Rust programming language\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPXS_h019bnT"
      },
      "outputs": [],
      "source": [
        "conversation_with_summary.predict(input=\"What's the best book to help me?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zJ1OQRH97Oc"
      },
      "outputs": [],
      "source": [
        "# Notice the buffer here is updated and clears the earlier exchanges\n",
        "conversation_with_summary.predict(input=\"Wish me luck!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell should generate a reply that is clearly generic and missing the previous context of someone trying to learn Rust.\n",
        "\n",
        "Run this cell, then go back to the `Keep the conversation going with summaries` cell and change `max_token_limit` to `700`. Then re-run the entire conversation and notice how the model relates its ouptut about learning C to the context of someone trying to learn Rust."
      ],
      "metadata": {
        "id": "McnotH6C5LOp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o08118VIgdzH"
      },
      "outputs": [],
      "source": [
        "conversation_with_summary.predict(input=\"Would knowing Haskell help me?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rkzR1SYAH09"
      },
      "source": [
        "### Summary\n",
        "\n",
        "In this notebook, we explored various approaches to memory in conversations.\n",
        "\n",
        "* ConversationBufferWindowMemory\n",
        "\n",
        "* ConversationSummaryBufferMemory\n",
        "\n",
        "* ConversationTokenBufferMemory\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPIUS58Y95aKGBBhAmTn4S",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}