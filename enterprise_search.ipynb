{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODJdxUeoE3l8Pald0S0T3C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rastringer/promptcraft/blob/main/enterprise_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enterprise search\n",
        "\n",
        "Using Langchain retrievers with [Enterprise Search](https://cloud.google.com/enterprise-search) on Google Cloud.\n",
        "\n",
        "As of July 2023, the product is available to trusted testers. This notebook offers an example use of retrieving relevant documents for a query.\n",
        "\n",
        "In this example, we will add course pdfs from Stanfords's CS224n class, which covers (rather aptly) NLP and LLMs. The dataset is available at `gs://cloud-samples-data/gen-app-builder/search/stanford-cs-224`.\n"
      ],
      "metadata": {
        "id": "2ii3ihksojvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade google-cloud-aiplatform\n",
        "! pip install google-cloud-discoveryengine\n",
        "! pip install shapely<2.0.0\n",
        "! pip install langchain\n",
        "! pip install typing-inspect==0.8.0 typing_extensions==4.5.0"
      ],
      "metadata": {
        "id": "A2k24lCDonBx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "040abc2a-7268-4071-c0a9-8d7cb7a70d02"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-cloud-aiplatform\n",
            "  Downloading google_cloud_aiplatform-1.28.1-py2.py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.11.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.22.3)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.20.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (23.1)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.10.0)\n",
            "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform)\n",
            "  Downloading google_cloud_resource_manager-1.10.2-py2.py3-none-any.whl (321 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.3/321.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shapely<2.0.0 (from google-cloud-aiplatform)\n",
            "  Downloading Shapely-1.8.5.post1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.59.1)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.17.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.27.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.56.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.48.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.4)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.5.0)\n",
            "Installing collected packages: shapely, google-cloud-resource-manager, google-cloud-aiplatform\n",
            "  Attempting uninstall: shapely\n",
            "    Found existing installation: shapely 2.0.1\n",
            "    Uninstalling shapely-2.0.1:\n",
            "      Successfully uninstalled shapely-2.0.1\n",
            "Successfully installed google-cloud-aiplatform-1.28.1 google-cloud-resource-manager-1.10.2 shapely-1.8.5.post1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-cloud-discoveryengine\n",
            "  Downloading google_cloud_discoveryengine-0.9.1-py3-none-any.whl (431 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.8/431.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-discoveryengine) (2.11.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-discoveryengine) (1.22.3)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-discoveryengine) (3.20.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-discoveryengine) (1.59.1)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-discoveryengine) (2.17.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-discoveryengine) (2.27.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-discoveryengine) (1.56.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-discoveryengine) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-discoveryengine) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-discoveryengine) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-discoveryengine) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-discoveryengine) (4.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-discoveryengine) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-discoveryengine) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-discoveryengine) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-discoveryengine) (3.4)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-discoveryengine) (0.5.0)\n",
            "Installing collected packages: google-cloud-discoveryengine\n",
            "Successfully installed google-cloud-discoveryengine-0.9.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: 2.0.0: No such file or directory\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.242-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.19)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.13-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.11 (from langchain)\n",
            "  Downloading langsmith-0.0.14-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.11)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, langsmith, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.13 langchain-0.0.242 langsmith-0.0.14 marshmallow-3.20.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0\n",
            "Collecting typing-inspect==0.8.0\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting typing_extensions==4.5.0\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect==0.8.0) (1.0.0)\n",
            "Installing collected packages: typing_extensions, typing-inspect\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.7.1\n",
            "    Uninstalling typing_extensions-4.7.1:\n",
            "      Successfully uninstalled typing_extensions-4.7.1\n",
            "  Attempting uninstall: typing-inspect\n",
            "    Found existing installation: typing-inspect 0.9.0\n",
            "    Uninstalling typing-inspect-0.9.0:\n",
            "      Successfully uninstalled typing-inspect-0.9.0\n",
            "Successfully installed typing-inspect-0.8.0 typing_extensions-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "OPxAFOghonE3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00af342a-332a-4adf-b8f2-f48b1b836ed7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're on Colab, authenticate via the following cell"
      ],
      "metadata": {
        "id": "hN7KknySjqov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "4BrhaYTBor1M"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add your `project id` and the `search engine id`. The search engine will have to be set up in the Google Cloud console. Future versions of the SDK should provide this feature."
      ],
      "metadata": {
        "id": "OqRMCpihjvxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"notebooks-370010\"\n",
        "SEARCH_ENGINE_ID = \"stanford_1690299721972\""
      ],
      "metadata": {
        "id": "uAn62-Fcottw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional parameters\n",
        "\n",
        "`max_documents` - The maximum number of documents used to provide extractive segments or extractive answers\n",
        "\n",
        "`get_extractive_answers` - By default, the retriever is configured to return extractive segments. Set this field to True to return extractive answers\n",
        "\n",
        "`max_extractive_answer_count` - The maximum number of extractive answers returned in each search result. At most 5 answers will be returned\n",
        "\n",
        "`max_extractive_segment_count` - The maximum number of extractive segments returned in each search result. Currently one segment will be returned\n",
        "\n",
        "`filter` - The filter expression that allows you filter the search results based on the metadata associated with the documents in the searched data store.\n",
        "\n",
        "`query_expansion_condition` - Specification to determine under which conditions query expansion should occur. 0 - Unspecified query expansion condition. In this case, server behavior defaults to disabled. 1 - Disabled query expansion. Only the exact search query is used, even if SearchResponse.total_size is zero. 2 - Automatic query expansion built by the Search API.\n"
      ],
      "metadata": {
        "id": "knVz9TV514i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import GoogleCloudEnterpriseSearchRetriever"
      ],
      "metadata": {
        "id": "DTWXJFdz2c05"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = GoogleCloudEnterpriseSearchRetriever(\n",
        "    project_id=PROJECT_ID,\n",
        "    search_engine_id=SEARCH_ENGINE_ID,\n",
        "    max_documents=3,\n",
        ")\n",
        "\n",
        "query = \"What are the goals of the course?\"\n",
        "\n",
        "result = retriever.get_relevant_documents(query)\n",
        "for doc in result:\n",
        "    print(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RPsYt9w2dph",
        "outputId": "67fceb27-1fa2-4279-d866-69e7584cb179"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='[draft] note 1: introduction and word2vec cs 224n: natural language processing with\\ndeep learning 3\\n\\navoid in this course. Partly, this is historical and methodological;\\nthe raw signal processing methods and expertise are generally\\ncovered in other courses (224s!) and other research communities,\\nthough there has been some convergence of techniques of late.\\nIn all aspects of NLP, most existing tools work for precious few (usu\\nally one, maybe up to 100) of the world’s roughly 7000 languages,\\nand fail disproportionately much on lesser-spoken and/or marginal\\nized dialects, accents, and more. Beyond this, recent successes in\\nbuilding better systems have far outstripped our ability to charac\\nterize and audit these systems. Biases encoded in text, from race to\\ngender to religion and more, are reflected and often amplified by\\nNLP systems. With these challenges and considerations in mind, but\\nwith the desire to do good science and build trustworthy systems\\nthat improve peoples’ lives, let’s take a look at a fascinating first\\nproblem in NLP.\\n\\n2 Representing words\\n2.1 Signifier and signified\\nConsider the sentence\\n\\nZuko makes the tea for his uncle.\\n\\nThe word Zuko is a sign, a symbol that represents an entity Zuko in\\nsome (real of imagined) world. The word tea is also a symbol that\\nrefers to a signified thing—perhaps a specific instance of tea. If one\\nwere instead to say Zuko likes to make tea for his uncle, note that the\\nsymbol Zuko still refers to Zuko, but now tea refers to a broader\\nclass—tea in general, not a specific bit of hot delicious water. Consider\\nthe two following sentences:\\n\\nZuko makes the coffee for his uncle.\\nZuko makes the drink for his uncle.\\n\\nWhich is “more like” the sentence about tea? The drink may be tea\\n(or it may be quite different!) and coffee definitely isn’t tea, but is yet\\nsimilar, no? And is Zuko similar to uncle because they both describe\\npeople? And is the similar to his because they both pick out specific\\ninstances of a class?\\nWord meaning is endlessly complex, deriving from humans’ goals\\nof communicating with each other and achieving goals in the world.\\nPeople use continuous media—speech, signing—but produce signs\\nin a discrete, symbolic structure—language—to express complex\\nmeanings. Expressing and processing the nuance and wildness of\\nlanguage—while achieving the strong transfer of information that' metadata={'source': 'gs://cloud-samples-data/gen-app-builder/search/stanford-cs-224/cs224n_winter2023_lecture1_notes_draft.pdf', 'id': '2f84b4522da1ad7216b708405a2e7fd1'}\n",
            "page_content='cs224n: natural language processing with deep learning lecture notes: part ii\\nword vectors ii: glove, evaluation and training 10\\n\\n3. Apply spherical k-means to cluster these context representations.\\n4. Finally, each word occurrence is re-labeled to its associated cluster\\nand is used to train the word representation for that cluster.\\nFor a more rigorous treatment on this topic, one should refer to\\nthe original paper.\\n\\n3 Training for Extrinsic Tasks\\nWe have so far focused on intrinsic tasks and emphasized their\\nimportance in developing a good word embedding technique. Of\\ncourse, the end goal of most real-world problems is to use the result\\ning word vectors for some other extrinsic task. Here we discuss the\\ngeneral approach for handling extrinsic tasks.\\n3.1 Problem Formulation\\n\\nMost NLP extrinsic tasks can be formulated as classification tasks.\\nFor instance, given a sentence, we can classify the sentence to have\\npositive, negative or neutral sentiment. Similarly, in named-entity\\nrecognition (NER), given a context and a central word, we want\\nto classify the central word to be one of many classes. For the in\\nput, \"Jim bought 300 shares of Acme Corp. in 2006\", we would\\nlike a classified output \"[Jim]Person bought 300 shares of [Acme\\nCorp.]Organization in [2006]Time.\"\\n\\nFigure 5: We can classify word vectors\\nusing simple linear decision boundaries\\nsuch as the one shown here (2-D word\\nvectors) using techniques such as\\nlogistic regression and SVMs\\n\\nFor such problems, we typically begin with a training set of the\\nform:\\n\\n{x (i)\\n\\n, y\\n\\n(i) }\\n\\nN\\n1\\n\\nwhere x\\n\\n(i) is a d-dimensional word vector generated by some word\\nembedding technique and y\\n(i) is a C-dimensional one-hot vector\\nwhich indicates the labels we wish to eventually predict (sentiments,\\nother words, named entities, buy/sell decisions, etc.).\\nIn typical machine learning tasks, we usually hold input data and\\ntarget labels fixed and train weights using optimization techniques\\n(such as gradient descent, L-BFGS, Newton’s method, etc.). In NLP\\napplications however, we introduce the idea of retraining the input\\nword vectors when we train for extrinsic tasks. Let us discuss when\\nand why we should consider doing this.\\n\\nImplementation Tip: Word vector\\nretraining should be considered for\\nlarge training datasets. For small\\ndatasets, retraining word vectors will\\nlikely worsen performance.' metadata={'source': 'gs://cloud-samples-data/gen-app-builder/search/stanford-cs-224/cs224n-2019-notes02-wordvecs2.pdf', 'id': '1ff1e859cfb0a87e309c43035be20ec9'}\n",
            "page_content='2. The Benchmark Tasks\\nIn this section, we briefly introduce four standard NLP tasks on which we will benchmark our\\narchitectures within this paper: Part-Of-Speech tagging (POS), chunking (CHUNK), Named Entity\\nRecognition (NER) and Semantic Role Labeling (SRL). For each of them, we consider a standard\\nexperimental setup and give an overview of state-of-the-art systems on this setup. The experimental\\nsetups are summarized in Table 1, while state-of-the-art systems are reported in Table 2.\\n2.1 Part-Of-Speech Tagging\\nPOS aims at labeling each word with a unique tag that indicates its syntactic role, for example, plural\\nnoun, adverb, . . .\\nA standard benchmark setup is described in detail by Toutanova et al. (2003).\\nSections 0–18 of Wall Street Journal (WSJ) data are used for training, while sections 19–21 are for\\nvalidation and sections 22–24 for testing.\\nThe best POS classifiers are based on classifiers trained on windows of text, which are then fed\\nto a bidirectional decoding algorithm during inference. Features include preceding and following\\n\\n2494' metadata={'source': 'gs://cloud-samples-data/gen-app-builder/search/stanford-cs-224/collobert11a.pdf', 'id': '3612967ed6bf4badc0bf4808a0b5eada'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = GoogleCloudEnterpriseSearchRetriever(\n",
        "    project_id=PROJECT_ID,\n",
        "    search_engine_id=SEARCH_ENGINE_ID,\n",
        "    max_documents=3,\n",
        "    max_extractive_answer_count=3,\n",
        "    get_extractive_answers=True,\n",
        ")"
      ],
      "metadata": {
        "id": "YgcwyxNl2qKE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Does the course cover transformers?\"\n",
        "\n",
        "result = retriever.get_relevant_documents(query)\n",
        "for doc in result:\n",
        "    print(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Usmuwl0I2shC",
        "outputId": "45afba42-d9b0-429e-b427-a39e6cbf82c2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='On faster GPUs, the pretraining can finish in around 30-40 minutes. This assignment is an investigation into Transformer self-attention building blocks, and the effects of pre training. It covers mathematical properties of Transformers and self-attention through written questions.' metadata={'source': 'gs://cloud-samples-data/gen-app-builder/search/stanford-cs-224/a5.pdf:2', 'id': 'e45a23e879587067446c6f876341de6d'}\n",
            "page_content='2. Pretrained Transformer models and knowledge access (35 points) You&#39;ll train a Transformer to perform a task that involves accessing knowledge about the world — knowledge which isn&#39;t provided via the task&#39;s training data (at least if you want to generalize outside the training set).' metadata={'source': 'gs://cloud-samples-data/gen-app-builder/search/stanford-cs-224/a5.pdf:2', 'id': 'e45a23e879587067446c6f876341de6d'}\n",
            "page_content='CS 224N Assignment 5 Page 2 of 10 1.' metadata={'source': 'gs://cloud-samples-data/gen-app-builder/search/stanford-cs-224/a5.pdf:2', 'id': 'e45a23e879587067446c6f876341de6d'}\n",
            "page_content='Image Transformer of parameters and consequently computational performance and can make training such models more challenging.' metadata={'source': 'gs://cloud-samples-data/gen-app-builder/search/stanford-cs-224/1802.05751.pdf:1', 'id': '0ed0222cb61a3398c75d4df1093e6562'}\n",
            "page_content='Our best CIFAR-10 model with DMOL has d and feed-forward layer layer dimension of 256 and perform attention in 512 dimensions.' metadata={'source': 'gs://cloud-samples-data/gen-app-builder/search/stanford-cs-224/1802.05751.pdf:1', 'id': '0ed0222cb61a3398c75d4df1093e6562'}\n",
            "page_content='Training recurrent neural networks to sequentially predict each pixel of even a small image is computationally very challenging.' metadata={'source': 'gs://cloud-samples-data/gen-app-builder/search/stanford-cs-224/1802.05751.pdf:1', 'id': '0ed0222cb61a3398c75d4df1093e6562'}\n",
            "page_content='In Chapter 10 we explored causal (left-to-right) transformers that can serve as the basis for powerful language models—models that can eas ily be applied to autoregressive generation problems such as contextual generation, summarization and machine translation.' metadata={'source': 'gs://cloud-samples-data/gen-app-builder/search/stanford-cs-224/11.pdf:10', 'id': '26b3bc83d90348c17ebad26a49853226'}\n",
            "page_content='Every input sentence first has to be tokenized, and then all further processing takes place on subword tokens rather than words.' metadata={'source': 'gs://cloud-samples-data/gen-app-builder/search/stanford-cs-224/11.pdf:10', 'id': '26b3bc83d90348c17ebad26a49853226'}\n",
            "page_content='11.3.1 Sequence Classification Sequence classification applications often represent an input sequence with a single consolidated representation.' metadata={'source': 'gs://cloud-samples-data/gen-app-builder/search/stanford-cs-224/11.pdf:10', 'id': '26b3bc83d90348c17ebad26a49853226'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is word2vec?\"\n",
        "\n",
        "result = retriever.get_relevant_documents(query)\n",
        "for doc in result:\n",
        "    print(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFw2eBGU4U53",
        "outputId": "bcf166ee-bf8e-4dd5-b112-f80ea9d0531d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='However, many of the details of word2vec will hold true in methods that we&#39;ll proceed to further in the course, so we&#39;ll focus our time on that. 3.2 Word2vec model and objective The word2vec model represents each word in a fixed vocabulary as a low-dimensional (much smaller than vocabulary size) vector.' metadata={'source': 'gs://cloud-samples-data/gen-app-builder/search/stanford-cs-224/cs224n_winter2023_lecture1_notes_draft.pdf:8', 'id': '2f84b4522da1ad7216b708405a2e7fd1'}\n",
            "page_content='[draft] note 1: introduction and word2vec cs 224n: natural language processing with deep learning 4 language is intended to achieve—makes representing words an endlessly fascinating problem. Let&#39;s move to some methods. 2.2 Independent words, independent vectors What is a word?' metadata={'source': 'gs://cloud-samples-data/gen-app-builder/search/stanford-cs-224/cs224n_winter2023_lecture1_notes_draft.pdf:8', 'id': '2f84b4522da1ad7216b708405a2e7fd1'}\n",
            "page_content='The word2vec model is a probabilistic model specified as follows, where uw refers to the row of U corresponding to word w ∈ V (and likewise for V): pU,V(o|c) = exp u ⊤ o vc ∑w∈V exp u⊤ w vc (4) This may be familiar to you as the softmax function, which takes arbitrary scores (here, one for each word in the vocabulary, resulting from dot products) and produces a probability distribution where larger-scored things get higher probability.' metadata={'source': 'gs://cloud-samples-data/gen-app-builder/search/stanford-cs-224/cs224n_winter2023_lecture1_notes_draft.pdf:8', 'id': '2f84b4522da1ad7216b708405a2e7fd1'}\n",
            "page_content='Although they learn word embeddings by optimizing over some objective functions using stochastic gradient methods, they have both been shown to be implicitly performing matrix factorizations. Skip-gram Skip-gram Word2Vec maximizes the likelihood of co-occurrence of the center word and context words.' metadata={'source': 'gs://cloud-samples-data/gen-app-builder/search/stanford-cs-224/NeurIPS-2018-on-the-dimensionality-of-word-embedding-Paper.pdf:2', 'id': '55b0877de28706e9b8ec530df0c60fa3'}\n",
            "page_content='For three popular embedding algorithms: LSA, skip-gram Word2Vec and GloVe, we find their optimal dimensionalities k ∗ that minimize their respective PIP loss. We define the sub-optimality of a particular dimensionality k as the additional PIP loss com pared with k ∗ : EkET k − EET − Ek∗Ek∗ T − EET .' metadata={'source': 'gs://cloud-samples-data/gen-app-builder/search/stanford-cs-224/NeurIPS-2018-on-the-dimensionality-of-word-embedding-Paper.pdf:2', 'id': '55b0877de28706e9b8ec530df0c60fa3'}\n",
            "page_content='It states that two embeddings are essentially identical if one can be obtained from the other by performing a unitary operation, eg, a rotation. A unitary operation on a vector corresponds to multiplying the vector by a unitary matrix, ie v = vU, where U TU = UUT = Id.' metadata={'source': 'gs://cloud-samples-data/gen-app-builder/search/stanford-cs-224/NeurIPS-2018-on-the-dimensionality-of-word-embedding-Paper.pdf:2', 'id': '55b0877de28706e9b8ec530df0c60fa3'}\n",
            "page_content='With word2vec, we train the skip-gram (SG† ) and continuous bag-of-words (CBOW† ) models on the 6 billion token corpus (Wikipedia 2014 + Giga word 5) with a vocabulary of the top 400000 most frequent words and a context window size of 10.' metadata={'source': 'gs://cloud-samples-data/gen-app-builder/search/stanford-cs-224/glove.pdf:10', 'id': '2d92bcf9f62444811e6640154cbe484e'}\n",
            "page_content='The most important remaining variable to con trol for is training time. For GloVe, the rele vant parameter is the number of training iterations. For word2vec, the obvious choice would be the number of training epochs. Unfortunately, the code is currently designed for only a single epoch:' metadata={'source': 'gs://cloud-samples-data/gen-app-builder/search/stanford-cs-224/glove.pdf:10', 'id': '2d92bcf9f62444811e6640154cbe484e'}\n",
            "page_content='We construct a model that utilizes this main ben efit of count data while simultaneously capturing the meaningful linear substructures prevalent in recent log-bilinear prediction-based methods like word2vec.' metadata={'source': 'gs://cloud-samples-data/gen-app-builder/search/stanford-cs-224/glove.pdf:10', 'id': '2d92bcf9f62444811e6640154cbe484e'}\n"
          ]
        }
      ]
    }
  ]
}