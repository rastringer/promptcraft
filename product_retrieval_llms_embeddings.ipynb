{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJydlOKumL3X"
      },
      "source": [
        "## Data Retrieval with LLMs and Embeddings\n",
        "\n",
        "Matching customer queries to products via embeddings and Retrieval Augmentated Generation.\n",
        "\n",
        "### Overview\n",
        "\n",
        "This notebook demonstrates one method of using large language models to interact with data. Using the Wayfair [WANDS](https://www.aboutwayfair.com/careers/tech-blog/wayfair-releases-wands-the-largest-and-richest-publicly-available-dataset-for-e-commerce-product-search-relevance) dataset of more than 42,000 products, we will go through the following steps:\n",
        "\n",
        "* Download the data into a pandas dataframe\n",
        "\n",
        "* Generate embeddings for the product descriptions\n",
        "\n",
        "* Create and deploy and index of the embeddings on Vertex AI Matching Engine, a service which enables [nearest neighbor](https://en.wikipedia.org/wiki/Nearest_neighbor_search) search at scale\n",
        "\n",
        "* Prompt an LLM to retrieve relevant product suggestions from the embedded data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvFqmT2lrkhO"
      },
      "source": [
        "<img src=\"https://assets.wfcdn.com/im/01139917/resize-h800-w800%5Ecompr-r85/2315/231567967/Capricornus+3+Seater+Sofa.jpg\" width=\"425\"/> <img src=\"https://assets.wfcdn.com/im/07725066/resize-h800-w800%5Ecompr-r85/1584/158440119/Vancasso+BOMOOTIUR+Stoneware+Dinnerware+-+Set+of+18.jpg\" width=\"425\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnDdmyF8sHf8"
      },
      "source": [
        "Images from wayfair.co.uk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW9JjlGt90nM"
      },
      "source": [
        "### Technologies\n",
        "\n",
        "In this notebook, we will use:\n",
        "\n",
        "* Vertex AI's language model\n",
        "\n",
        "* Vertex AI [Matching Engine](https://cloud.google.com/vertex-ai/docs/matching-engine/overview), a high-scale, low-latency vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vh1uCga9mi-S"
      },
      "outputs": [],
      "source": [
        "# Install the packages\n",
        "! pip3 install --upgrade google-cloud-aiplatform\n",
        "! pip3 install shapely<2.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwX0d7Whtokk"
      },
      "source": [
        "\n",
        "### Colab only: Uncomment the following cell to restart the kernel\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7oPa-QPtlJ1"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLftnOVnnuLA"
      },
      "source": [
        "Set your Google Cloud project id and region"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbJqgT5ttnmE"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"<...>\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wC3Kr6g9txMd"
      },
      "outputs": [],
      "source": [
        "REGION = \"<...>\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euWhNYGSnxbJ"
      },
      "source": [
        "We will need a Cloud Storage bucket to store embeddings initially. Please create a bucket and add the URI below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYXAXFdHuabg"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = \"gs://<...>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nngKIrTCt2jW"
      },
      "source": [
        "Authenticate your Google Cloud account\n",
        "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcmNX7zot6vG"
      },
      "source": [
        "1. Vertex AI Workbench\n",
        "\n",
        "Do nothing as you are already authenticated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anclot6wuAFO"
      },
      "source": [
        "2. Local JupyterLab instance, uncomment and run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzCFhkJut5fm"
      },
      "outputs": [],
      "source": [
        "# ! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3N_HToruGy2"
      },
      "source": [
        "3. Colab, uncomment and run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PGJV1uguFCX"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOLFzskAn9gT"
      },
      "source": [
        "Install and intialize the SDK and language model. GCP uses the `gecko` model for text embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRczTZcpuWpG"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDOXkjQ_uI73"
      },
      "outputs": [],
      "source": [
        "# Load the \"Vertex AI Embeddings for Text\" model\n",
        "from vertexai.preview.language_models import TextEmbeddingModel\n",
        "\n",
        "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2s5GDO1u7Zr"
      },
      "source": [
        "Now we're ready to prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5w9wKbvuP8o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "path = \"data\"\n",
        "\n",
        "os.path.exists(path)\n",
        "if not os.path.exists(path):\n",
        "  os.makedirs(path)\n",
        "  print(\"data directory created\")\n",
        "else:\n",
        "  print(\"data directory found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uxIYbc1vdFX"
      },
      "outputs": [],
      "source": [
        "# download datasets\n",
        "!wget -q https://raw.githubusercontent.com/wayfair/WANDS/main/dataset/product.csv\n",
        "\n",
        "!mv *.csv data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXyi6CCpvyJT"
      },
      "outputs": [],
      "source": [
        "!ls data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emlCzIIVoYs-"
      },
      "source": [
        "The dataset features a wealth of information. The queries (user searchers), and the rating of the responses to the queries, have been particularly interesting to researchers. For this demo however we will focus on the product descriptions.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7zTwjBiwVea"
      },
      "outputs": [],
      "source": [
        "product_df = pd.read_csv(\"data/product.csv\", sep='\\t')\n",
        "product_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjQWa0gqozBa"
      },
      "source": [
        "Filter the dataframe to consider `product_id`, `product_name`, `product_description`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcvSG56SwaRr"
      },
      "outputs": [],
      "source": [
        "product_df = product_df.filter([\"product_id\", \"product_name\", \"product_description\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7V3Y5OAwcCM"
      },
      "outputs": [],
      "source": [
        "product_df = product_df.rename(columns={\"product_description\": \"product_text\", \"product_id\": \"id\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URmCaKr3wdmM"
      },
      "outputs": [],
      "source": [
        "product_df = product_df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s20fIeDRUsaI"
      },
      "outputs": [],
      "source": [
        "len(product_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YnkZcTLpMcj"
      },
      "source": [
        "The following three cells contain functions from this [notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/matching_engine/sdk_matching_engine_create_stack_overflow_embeddings_vertex.ipynb) from the vertex-ai-samples repository.\n",
        "\n",
        "`encode_texts_to_embeddings` will be used later to convert the product descriptions into   embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zz85FRvGwg_9"
      },
      "outputs": [],
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "# Define an embedding method that uses the model\n",
        "def encode_texts_to_embeddings(text: List[str]) -> List[Optional[List[float]]]:\n",
        "    try:\n",
        "        embeddings = model.get_embeddings(text)\n",
        "        return [embedding.values for embedding in embeddings]\n",
        "    except Exception:\n",
        "        return [None for _ in range(len(text))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djr6MMCQpgRE"
      },
      "source": [
        "These helper functions achieve the following:\n",
        "\n",
        "* `generate_batches` splits the product descriptions into batches of five, since the embeddings API will field up to five text instances in each request.\n",
        "\n",
        "* `encode_text_to_embedding_batched` calls the embeddings API and handles rate limiting using `time.sleep`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVjTTeCFxjM7"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import Generator, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "# Generator function to yield batches of sentences\n",
        "def generate_batches(\n",
        "    text: List[str], batch_size: int\n",
        ") -> Generator[List[str], None, None]:\n",
        "    for i in range(0, len(text), batch_size):\n",
        "        yield text[i : i + batch_size]\n",
        "\n",
        "\n",
        "def encode_text_to_embedding_batched(\n",
        "    text: List[str], api_calls_per_second: int = 10, batch_size: int = 5\n",
        ") -> Tuple[List[bool], np.ndarray]:\n",
        "\n",
        "    embeddings_list: List[List[float]] = []\n",
        "\n",
        "    # Prepare the batches using a generator\n",
        "    batches = generate_batches(text, batch_size)\n",
        "\n",
        "    seconds_per_job = 1 / api_calls_per_second\n",
        "\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        futures = []\n",
        "        for batch in tqdm(\n",
        "            batches, total=math.ceil(len(text) / batch_size), position=0\n",
        "        ):\n",
        "            futures.append(\n",
        "                executor.submit(functools.partial(encode_texts_to_embeddings), batch)\n",
        "            )\n",
        "            time.sleep(seconds_per_job)\n",
        "\n",
        "        for future in futures:\n",
        "            embeddings_list.extend(future.result())\n",
        "\n",
        "    is_successful = [\n",
        "        embedding is not None for text, embedding in zip(text, embeddings_list)\n",
        "    ]\n",
        "    embeddings_list_successful = np.squeeze(\n",
        "        np.stack([embedding for embedding in embeddings_list if embedding is not None])\n",
        "    )\n",
        "    return is_successful, embeddings_list_successful"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzkSUOazqAY4"
      },
      "source": [
        "Let's encode a subset of data and check the distance metrics provide sane product suggestions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv2HM2DrxtSn"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# Encode a subset of questions for validation\n",
        "products = product_df.product_text.tolist()[:500]\n",
        "is_successful, product_embeddings = encode_text_to_embedding_batched(\n",
        "    text=product_df.product_text.tolist()[:500]\n",
        ")\n",
        "\n",
        "# Filter for successfully embedded sentences\n",
        "products = np.array(products)[is_successful]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yf0bQU29wit-"
      },
      "outputs": [],
      "source": [
        "DIMENSIONS = len(product_embeddings[0])\n",
        "\n",
        "print(DIMENSIONS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKrjOKlxqSqg"
      },
      "source": [
        "This function takes a description from the dataset (rather than a user) and looks for relevant matches. The first answer is likely to be the exact match."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXg_ENyXxFyx"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "product_index = random.randint(0, 99)\n",
        "\n",
        "print(f\"Product query: {products[product_index]} \\n\")\n",
        "\n",
        "scores = np.dot(product_embeddings[product_index], product_embeddings.T)\n",
        "\n",
        "# Print top 3 matches\n",
        "for index, (product, score) in enumerate(\n",
        "    sorted(zip(products, scores), key=lambda x: x[1], reverse=True)[:3]\n",
        "):\n",
        "    print(f\"\\t{index}: \\n {product}: \\n {score} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuLz_s2mH6TL"
      },
      "source": [
        "### Data formatting for building an index\n",
        "\n",
        "We need to save the embeddings and the `id` and `product_name` columns to the JSON lines format in order to creat an index on Matching Engine. For more details, see the documentation [here](https://cloud.google.com/vertex-ai/docs/matching-engine/match-eng-setup/format-structure)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUwxakjsBfmv"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Create temporary file to write embeddings to\n",
        "embeddings_file_path = Path(tempfile.mkdtemp())\n",
        "\n",
        "print(f\"Embeddings directory: {embeddings_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktI9CQ7VVjo3"
      },
      "outputs": [],
      "source": [
        "product_embeddings = np.array(product_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKrlms3sXT2-"
      },
      "outputs": [],
      "source": [
        "!touch json_output.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6Q54JpKrGLO"
      },
      "source": [
        "Let's take a look at the shape and type of the embeddings. At the moment, the `product_embeddings` are a numpy array. We will need to convert them to a Python dictionary to use them as another column in a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNM9abfQj2-D"
      },
      "outputs": [],
      "source": [
        "type(product_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_lyi01ikqrE"
      },
      "outputs": [],
      "source": [
        "embeddings_list = product_embeddings.tolist()\n",
        "embeddings_dicts = [{'embedding': embedding} for embedding in embeddings_list]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-iPw0vgkuMa"
      },
      "outputs": [],
      "source": [
        "embeddings_df = product_df.merge(pd.DataFrame(embeddings_dicts), left_on='id', right_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HdacepKTfGc"
      },
      "outputs": [],
      "source": [
        "embeddings_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R0TIYnRr9x7"
      },
      "source": [
        "### JSON Lines\n",
        "Now we can convert the entire dataframe to JSON lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzXgfRsmlGYX"
      },
      "outputs": [],
      "source": [
        "json_lines = embeddings_df.to_json(orient='records', lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AfdjIEllIQJ"
      },
      "outputs": [],
      "source": [
        "json_lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g97aGu8Nl1b3"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "output_file = 'merged_data.json'\n",
        "with open(output_file, 'w') as file:\n",
        "    for index, row in embeddings_df.iterrows():\n",
        "        data = {\n",
        "            'id': row['id'],\n",
        "            'product_name': row['product_name'],\n",
        "            'product_text': row['product_text'],\n",
        "            'embedding': row['embedding']\n",
        "        }\n",
        "        json_line = json.dumps(data)\n",
        "        file.write(json_line + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvPJlmcUsGRT"
      },
      "source": [
        "Copy the JSON lines file to Cloud Storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJA2HZumXhjG"
      },
      "outputs": [],
      "source": [
        "!gsutil cp merged_data.json gs://genai-experiments/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ug8lDzztQutq"
      },
      "outputs": [],
      "source": [
        "!cat json_output.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zebZZzZdsj2O"
      },
      "source": [
        "### Creating the index in Matching Engine\n",
        "\n",
        "*This is a long-running operation which can take up to an hour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQWDg7dgLg-3"
      },
      "outputs": [],
      "source": [
        "DIMENSIONS = 768\n",
        "# Add a display name\n",
        "DISPLAY_NAME = \"wands_index\"\n",
        "DESCRIPTION = \"products and descriptions from Wayfair\"\n",
        "remote_folder = BUCKET_URI\n",
        "\n",
        "tree_ah_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
        "    display_name=DISPLAY_NAME,\n",
        "    contents_delta_uri=remote_folder,\n",
        "    dimensions=DIMENSIONS,\n",
        "    approximate_neighbors_count=150,\n",
        "    distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
        "    leaf_node_embedding_count=500,\n",
        "    leaf_nodes_to_search_percent=5,\n",
        "    description=DESCRIPTION,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xe-sXKlt3BB"
      },
      "source": [
        "In the results of the cell above, make note of the information under this line:\n",
        "\n",
        "*To use this MatchingEngineIndex in another session*:\n",
        "\n",
        "If Colab runtime resets, you will need this line to set the index variable:\n",
        "\n",
        "`\n",
        "index = aiplatform.MatchingEngineIndex(...)\n",
        "`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Glv5TS2ks_lS"
      },
      "source": [
        "Use `gcloud` to list indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBq3yCpOQ_z7"
      },
      "outputs": [],
      "source": [
        "# Add your region below\n",
        "!gcloud ai indexes list --region=\"<...>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbL98ybcW8Zs"
      },
      "outputs": [],
      "source": [
        "INDEX_RESOURCE_NAME = tree_ah_index.resource_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnzVseQAuQZa"
      },
      "source": [
        "### Deploy the index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgyMXiWWW_3N"
      },
      "outputs": [],
      "source": [
        "my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
        "    display_name=DISPLAY_NAME,\n",
        "    description=DISPLAY_NAME,\n",
        "    public_endpoint_enabled=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vno0zCpdNRHl"
      },
      "source": [
        "* Note, here is how to get an existing\n",
        "`MatchingEngineIndex` (from the output in the MatchingEngineIndex.create cell above) and\n",
        " `MatchingEngineIndexEndpoint` (from another project, or if the Colab runtime resets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlmolTKrNQGu"
      },
      "outputs": [],
      "source": [
        "# Fill in the values from the MatchingEngineIndex.create\n",
        "# and MatchingEngineIndexEndpoint.create cells\n",
        "\n",
        "# index = aiplatform.MatchingEngineIndex('<...>')\n",
        "\n",
        "# my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(\n",
        "#     index_endpoint_name = '<...>',\n",
        "# )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0dPhLkkXGFT"
      },
      "outputs": [],
      "source": [
        "# Write your own unique index name\n",
        "DEPLOYED_INDEX_ID = \"<...>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDNWqWPj_wGU"
      },
      "source": [
        "### Deploy the index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dORsb5IRXH-t"
      },
      "outputs": [],
      "source": [
        "my_index_endpoint = my_index_endpoint.deploy_index(\n",
        "    index=index, deployed_index_id=DEPLOYED_INDEX_ID\n",
        ")\n",
        "\n",
        "my_index_endpoint.deployed_indexes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1cCOAlJuqW9"
      },
      "source": [
        "### Quick test query\n",
        "\n",
        "Embedding a query should return relevant nearest neighbors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ookuOg79XL8e"
      },
      "outputs": [],
      "source": [
        "test_embeddings = encode_texts_to_embeddings(text=[\"a midcentury modern dining table\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJlnI2FJXRcO"
      },
      "outputs": [],
      "source": [
        "# Test query\n",
        "NUM_NEIGHBOURS = 5\n",
        "\n",
        "response = my_index_endpoint.find_neighbors(\n",
        "    deployed_index_id=DEPLOYED_INDEX_ID,\n",
        "    queries=test_embeddings,\n",
        "    num_neighbors=NUM_NEIGHBOURS,\n",
        ")\n",
        "\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKvo0HMsu0oe"
      },
      "source": [
        "Now let's make that information useful, by creating helper functions to take the `id`s and match them to products."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjEwZHH_i4HB"
      },
      "outputs": [],
      "source": [
        "# Get the ids of the nearest neighbor results\n",
        "\n",
        "def get_nn_ids(response):\n",
        "  id_list = [item.id for sublist in response for item in sublist]\n",
        "  id_list = [eval(i) for i in id_list]\n",
        "  print(id_list)\n",
        "  results_df = product_df[product_df['id'].isin(id_list)]\n",
        "  return results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTeG5HpRpATF"
      },
      "outputs": [],
      "source": [
        "# Create embeddings from a customer chat message\n",
        "\n",
        "def get_embeddings(input_text):\n",
        "  chat_embeddings = encode_texts_to_embeddings(text=[input_text])\n",
        "\n",
        "  return chat_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZhDKnl8p-p7"
      },
      "outputs": [],
      "source": [
        "# Retrieve the nearest neighbor lookups for\n",
        "# the embedded customer message\n",
        "\n",
        "NUM_NEIGHBOURS = 3\n",
        "\n",
        "def get_nn_response(chat_embeddings):\n",
        "  response = my_index_endpoint.find_neighbors(\n",
        "    deployed_index_id=DEPLOYED_INDEX_ID,\n",
        "    queries=chat_embeddings,\n",
        "    num_neighbors=NUM_NEIGHBOURS,\n",
        ")\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgOXIMxfqLpz"
      },
      "outputs": [],
      "source": [
        "# Create a dataframe of results. This will be the data we\n",
        "# ask the language model to base its recommendations on\n",
        "\n",
        "def get_nn_ids(response):\n",
        "  id_list = [item.id for sublist in response for item in sublist]\n",
        "  id_list = [eval(i) for i in id_list]\n",
        "  print(id_list)\n",
        "  results_df = product_df[product_df['id'].isin(id_list)]\n",
        "\n",
        "  return results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wCJ2jzYviqt"
      },
      "source": [
        "### RAG using the LLM and embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XpvwrXYf7Hl"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "from vertexai.preview.language_models import ChatModel, InputOutputTextPair\n",
        "\n",
        "chat_model = ChatModel.from_pretrained(\"chat-bison@001\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.1,\n",
        "    \"max_output_tokens\": 1024,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "\n",
        "customer_message = \"\"\"\\\n",
        "Interested in a persian style rug\n",
        "\"\"\"\n",
        "\n",
        "# Chain together the helper functions to get results\n",
        "# from customer_message\n",
        "results_df = get_nn_ids(get_nn_response(get_embeddings(customer_message)))\n",
        "\n",
        "service_context=f\"\"\"You are a customer service bot, writing in polite British English. \\\n",
        "    Suggest the top three relevant \\\n",
        "    products only from {results_df}, mentioning:\n",
        "     product names and \\\n",
        "     brief descriptions \\\n",
        "    Number them and leave a line between suggestions. \\\n",
        "    Preface the list of products with an introductory sentence such as \\\n",
        "    'Here are some relevant products: ' \\\n",
        "    Ensure each recommendation appears only once.\"\"\"\n",
        "\n",
        "\n",
        "chat = chat_model.start_chat(\n",
        "    context=f\"\"\"{service_context}\"\"\",\n",
        ")\n",
        "response = chat.send_message(customer_message, **parameters)\n",
        "print(f\"Response from Model: \\n {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuvGNffav1_O"
      },
      "source": [
        "A user may ask follow up questions, which the LLM could answer based on the information in the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Q3BtPh2ngfQ"
      },
      "outputs": [],
      "source": [
        "response = chat.send_message(\"\"\"could you tell me more about the Octagon Senoia?\"\"\", **parameters)\n",
        "print(f\"Response from Model: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mSWOwzPwIgW"
      },
      "source": [
        "### Cleaning up\n",
        "\n",
        "To delete all the GCP resources used, uncomment and run the following cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ta6vshItqZQ5"
      },
      "outputs": [],
      "source": [
        "# Force undeployment of indexes and delete endpoint\n",
        "# my_index_endpoint.delete(force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKmCrc0fwUhK"
      },
      "outputs": [],
      "source": [
        "# Delete indexes\n",
        "# tree_ah_index.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
