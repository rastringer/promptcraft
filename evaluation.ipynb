{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rastringer/promptcraft_notebooks/blob/main/evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac4b9f54",
      "metadata": {
        "id": "ac4b9f54"
      },
      "source": [
        "## Evaluating outputs\n",
        "\n",
        "In this notebook we will explore using the model to evaluate the quality and relevance of its outputs. This may seem meta, however, extracting responses into variables and asking follow-up questions with correct instructions can be an accurate and simple way of checking performance.\n",
        "\n",
        "We're importing the various helper functions from the last notebook from `helper_functions.py`, and our products are in a separate `products.json` file."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c381b32b",
      "metadata": {
        "id": "c381b32b"
      },
      "source": [
        "If you're on Colab, run the following cell to authenticate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3678fb67",
      "metadata": {
        "id": "3678fb67"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c9ad705",
      "metadata": {
        "id": "9c9ad705"
      },
      "outputs": [],
      "source": [
        "from helper_functions import *\n",
        "from google.cloud import aiplatform as vertexai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50072c8f",
      "metadata": {
        "id": "50072c8f"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "from vertexai.preview.language_models import ChatModel, InputOutputTextPair\n",
        "\n",
        "# Replace the project and location placeholder values below\n",
        "vertexai.init(project=\"<your-project-id>\", location=\"<location>\")\n",
        "chat_model = ChatModel.from_pretrained(\"chat-bison@001\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 1024,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e294a3d9",
      "metadata": {
        "id": "e294a3d9"
      },
      "source": [
        "### Set up\n",
        "\n",
        "Once again, let's run the user query and extract the product information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eacdcc16",
      "metadata": {
        "id": "eacdcc16"
      },
      "outputs": [],
      "source": [
        "context = f\"\"\"\n",
        "You're a customer service assistant for a coffee shop's \\\n",
        "e-commerce site. Our product list can be found in {products}. Respond in a friendly and professional \\\n",
        "tone with concise answers. \\\n",
        "Please ask the user relevant follow-up questions.\n",
        "\"\"\"\n",
        "\n",
        "user_message_1 = f\"\"\"\n",
        "Tell me about the Brew Blend pro and \\\n",
        "the stovetop coffee maker. \\\n",
        "I'm also interested in espresso machines.\"\"\"\n",
        "\n",
        "chat = chat_model.start_chat(\n",
        "    context=context,\n",
        "    examples=[]\n",
        ")\n",
        "\n",
        "assistant_response = chat.send_message(user_message_1, **parameters)\n",
        "print(assistant_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fca7e72c",
      "metadata": {
        "id": "fca7e72c"
      },
      "source": [
        "We can then convert the text response into a product list. This function will be hidden from the user.\n",
        "We can then use this product list to check the relevance of our recommendations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2256d6e6",
      "metadata": {
        "id": "2256d6e6"
      },
      "outputs": [],
      "source": [
        "context = f\"\"\"\n",
        "Take as input the {assistant_response} and output a python dictionary of objects, \\\n",
        "where each object has \\\n",
        "the following format:\n",
        "    'category': <one of \\\n",
        "    Espresso Machines, \\\n",
        "    Single Serve Coffee Makers, \\\n",
        "    Drip Coffee Makers, \\\n",
        "    Stovetop Coffee Makers,\n",
        "    Coffee and Espresso Combo Machines>,\n",
        "AND\n",
        "    'products': <a list of products that must \\\n",
        "    be found in the allowed products below>\n",
        "\n",
        "For example,\n",
        "  'category': 'Coffee and Espresso Combo Machines', 'products': ['AeroBlend Max'],\n",
        "\n",
        "Where the categories and products must be found in \\\n",
        "the customer service query.\n",
        "If a product is mentioned, it must be associated with \\\n",
        "the correct category in the allowed products list below.\n",
        "If no products or categories are found, output an \\\n",
        "empty list.\n",
        "\n",
        "Allowed products:\n",
        "\n",
        "Espresso Machines category:\n",
        "Caffeino Classic\n",
        "\n",
        "Single Serve Coffee Makers:\n",
        "BeanPresso\n",
        "\n",
        "Drip Coffee Makers:\n",
        "BrewBlend Pro\n",
        "\n",
        "Stovetop Coffee Makers:\n",
        "SteamGenie\n",
        "\n",
        "Coffee and Espresso Combo Machines:\n",
        "AeroBlend Max\n",
        "\n",
        "Only output the list of objects, with nothing else.\n",
        "\"\"\"\n",
        "\n",
        "chat = chat_model.start_chat(\n",
        "    context=context,\n",
        "    examples=[]\n",
        ")\n",
        "\n",
        "products_response = chat.send_message(user_message_1)\n",
        "print(products_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "564fc0ba",
      "metadata": {
        "id": "564fc0ba"
      },
      "outputs": [],
      "source": [
        "temp_str = str(products_response)\n",
        "category_and_product_list = read_string_to_list(temp_str)\n",
        "category_and_product_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27a38a65",
      "metadata": {
        "id": "27a38a65"
      },
      "outputs": [],
      "source": [
        "product_info_for_user_message_1 = generate_output_string(category_and_product_list)\n",
        "print(product_info_for_user_message_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bb3e306",
      "metadata": {
        "id": "0bb3e306"
      },
      "source": [
        "### Check output\n",
        "\n",
        "Now that we have our outputs as handly lists and strings, we can add them as inputs for the model to check. This step will become less necessary as models become more sophisticated, and is only recommended for extremely highly sensitive applications since it adds cost and latency and may be unnecessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eff7aa41",
      "metadata": {
        "id": "eff7aa41"
      },
      "outputs": [],
      "source": [
        "context = f\"\"\"\n",
        "You are an assistant that evaluates whether \\\n",
        "customer service agent responses sufficiently \\\n",
        "answer customer questions, and also validates that \\\n",
        "all the facts the assistant cites from the product \\\n",
        "information are correct.\n",
        "The product information and user and customer \\\n",
        "service agent messages will be delimited by \\\n",
        "3 backticks, i.e. ```.\n",
        "Respond with a Y or N character, with no punctuation:\n",
        "Y - if the output sufficiently answers the question \\\n",
        "AND the response correctly uses product information\n",
        "N - otherwise\n",
        "\n",
        "Output a single letter only.\n",
        "\"\"\"\n",
        "customer_message = f\"\"\"\n",
        "Tell me all about the Brew Blend pro and \\\n",
        "the stovetop coffee maker - features and pricing. \\\n",
        "I'm also interested in an espresso machine\"\"\"\n",
        "\n",
        "q_a_pair = f\"\"\"\n",
        "Customer message: ```{customer_message}```\n",
        "Product information: ```{product_info_for_user_message_1}```\n",
        "Agent response: ```{assistant_response}```\n",
        "\n",
        "Does the response use the retrieved information correctly?\n",
        "Does the response sufficiently answer the question\n",
        "\n",
        "Output Y or N\n",
        "\"\"\"\n",
        "\n",
        "chat = chat_model.start_chat(\n",
        "    context=context,\n",
        "    examples=[]\n",
        ")\n",
        "\n",
        "response = chat.send_message(f\"\"\"{q_a_pair}\"\"\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b500dcd3",
      "metadata": {
        "id": "b500dcd3"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "561c8002",
      "metadata": {
        "id": "561c8002"
      },
      "outputs": [],
      "source": [
        "def eval_with_rubric(customer_message, assistant_response):\n",
        "\n",
        "    customer_message = f\"\"\"\n",
        "    Tell me all about the Brew Blend pro and \\\n",
        "    the stovetop coffee maker - features and pricing. \\\n",
        "    I'm also interested in an espresso machine.\"\"\"\n",
        "\n",
        "    context = \"\"\"\\\n",
        "    You are an assistant that evaluates how well the customer service agent \\\n",
        "    answers a user question by looking at the context that the customer service \\\n",
        "    agent is using to generate its response.\n",
        "    Compare the factual content of the submitted answer with the context. \\\n",
        "    Ignore any differences in style, grammar, or punctuation.\n",
        "    Answer the following questions:\n",
        "        - Is the Assistant response based only on the context provided? (Y or N)\n",
        "        - Does the answer include information that is not provided in the context? (Y or N)\n",
        "        - Is there any disagreement between the response and the context? (Y or N)\n",
        "        - Count how many questions the user asked. (output a number)\n",
        "        - For each question that the user asked, is there a corresponding answer to it?\n",
        "          Question 1: (Y or N)\n",
        "          Question 2: (Y or N)\n",
        "          ...\n",
        "          Question N: (Y or N)\n",
        "        - Of the number of questions asked, how many of these questions were addressed by the answer? (output a number)\n",
        "    \"\"\"\n",
        "\n",
        "    user_message = f\"\"\"\\\n",
        "    You are evaluating a submitted answer to a question based on the context \\\n",
        "    that the agent uses to answer the question.\n",
        "    Here is the data:\n",
        "    [BEGIN DATA]\n",
        "    ************\n",
        "    [Question]: {customer_message}\n",
        "    ************\n",
        "    [Context]: {context}\n",
        "    ************\n",
        "    [Submission]: {assistant_response}\n",
        "    ************\n",
        "    [END DATA]\n",
        "\"\"\"\n",
        "    chat = chat_model.start_chat(\n",
        "    context=context,\n",
        "    examples=[]\n",
        "    )\n",
        "\n",
        "    response = chat.send_message(user_message, max_output_tokens=1024)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3e4d066",
      "metadata": {
        "id": "f3e4d066"
      },
      "outputs": [],
      "source": [
        "product_info = product_info_for_user_message_1\n",
        "\n",
        "customer_product_info = {\n",
        "    \"customer_message\": customer_message,\n",
        "    \"context\": product_info\n",
        "}\n",
        "eval_output = eval_with_rubric(customer_product_info, assistant_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1bf5ec6",
      "metadata": {
        "id": "d1bf5ec6"
      },
      "outputs": [],
      "source": [
        "print(eval_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dfd9d60",
      "metadata": {
        "id": "2dfd9d60"
      },
      "source": [
        "### Evaluate based on an expert human answer\n",
        "\n",
        "We can write our own example of what an excellent human answer would be, then ask the model to compare its responses with our example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ba6470c",
      "metadata": {
        "id": "6ba6470c"
      },
      "outputs": [],
      "source": [
        "ideal_example = {\n",
        "    'customer_message': \"\"\"\\\n",
        "    Tell me all about the Brew Blend pro and \\\n",
        "    the stovetop coffee maker - features and pricing. \\\n",
        "    I'm also interested in an espresso machine?\"\"\",\n",
        "\n",
        "    'ideal_answer': \"\"\"\\\n",
        "    Of course! The BrewBlend pro is a powerhouse of a drip coffee maker. \\\n",
        "    The BrewBlend offers a superior brewing experience with adjustable \\\n",
        "    brew strength, and anti-drip system. \\\n",
        "    Love your coffee first thing when you wake up? Just set the programmable \\\n",
        "    timer. It's priced at 389.99. \\\n",
        "    The stovetop option is the SteamGenie, a coffee maker crafted with \\\n",
        "    durable stainless steel. The SteamGenie delivers a rich, strong and authentic \\\n",
        "    coffee experience with every brew. \\\n",
        "    We do have an espresso machine, the Caffeino Classic. It's a 15-bar \\\n",
        "    pump for authentic espresso extraction, wiht a milk frother and \\\n",
        "    water reservoir for easy refiling. It costs 179.99.\n",
        "    \"\"\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3be651a4",
      "metadata": {
        "id": "3be651a4"
      },
      "source": [
        "### Evals\n",
        "\n",
        "There are scoring systems such as *Bleu* that researchers have used to check model performance for language tasks. Another approach is to use OpenAI's [evals framework](https://github.com/openai/evals), from which the following grading criteria are used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b73b8ab3",
      "metadata": {
        "id": "b73b8ab3"
      },
      "outputs": [],
      "source": [
        "def eval_vs_ideal(ideal_example, assistant_response):\n",
        "\n",
        "    customer_message = ideal_example['customer_message']\n",
        "    ideal_answer = ideal_example['ideal_answer']\n",
        "    completion = assistant_response\n",
        "\n",
        "    context = \"\"\"\\\n",
        "    You are an assistant that evaluates how well the customer service agent \\\n",
        "    answers a user question by comparing the response to the ideal (expert) response\n",
        "    Output a single letter and nothing else.\n",
        "    Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.\n",
        "    The submitted answer may either be a subset or superset of the expert answer, or it may conflict with it. Determine which case applies. Answer the question by selecting one of the following options:\n",
        "    (A) The submitted answer is a subset of the expert answer and is fully consistent with it.\n",
        "    (B) The submitted answer is a superset of the expert answer and is fully consistent with it.\n",
        "    (C) The submitted answer contains all the same details as the expert answer.\n",
        "    (D) There is a disagreement between the submitted answer and the expert answer.\n",
        "    (E) The answers differ, but these differences don't matter from the perspective of factuality.\n",
        "  choice_strings: ABCDE\n",
        "    \"\"\"\n",
        "\n",
        "    user_message = f\"\"\"\\\n",
        "You are comparing a submitted answer to an expert answer on a given question. Here is the data:\n",
        "    [BEGIN DATA]\n",
        "    ************\n",
        "    [Question]: {customer_message}\n",
        "    ************\n",
        "    [Expert]: {ideal_answer}\n",
        "    ************\n",
        "    [Submission]: {completion}\n",
        "    ************\n",
        "    [END DATA]\n",
        "\"\"\"\n",
        "\n",
        "    chat = chat_model.start_chat(\n",
        "    context=context,\n",
        "    examples=[]\n",
        "    )\n",
        "\n",
        "    response = chat.send_message(user_message, max_output_tokens=1024)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9198bf3",
      "metadata": {
        "id": "e9198bf3"
      },
      "outputs": [],
      "source": [
        "eval_vs_ideal(ideal_example, assistant_response)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}